{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tempfile\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from filelock import FileLock\n",
    "from torch.utils.data import random_split\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from typing import Dict\n",
    "import ray\n",
    "from ray import train, tune\n",
    "from ray.train import Checkpoint\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "RANDOM_SEED = 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "NVIDIA GeForce RTX 4080 SUPER\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir=\"./data\"):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    # We add FileLock here because multiple workers will want to\n",
    "    # download data, and this may cause overwrites since\n",
    "    # DataLoader is not threadsafe.\n",
    "    with FileLock(os.path.expanduser(\"~/.data.lock\")):\n",
    "        trainset = torchvision.datasets.CIFAR10(\n",
    "            root=data_dir, train=True, download=True, transform=transform)\n",
    "\n",
    "        testset = torchvision.datasets.CIFAR10(\n",
    "            root=data_dir, train=False, download=True, transform=transform)\n",
    "\n",
    "    return trainset, testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_data():\n",
    "    # Load fake data for running a quick smoke-test.\n",
    "    trainset = torchvision.datasets.FakeData(\n",
    "        128, (3, 32, 32), num_classes=10, transform=transforms.ToTensor()\n",
    "    )\n",
    "    testset = torchvision.datasets.FakeData(\n",
    "        16, (3, 32, 32), num_classes=10, transform=transforms.ToTensor()\n",
    "    )\n",
    "    return trainset, testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, l1=120, l2=84):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, l1)\n",
    "        self.fc2 = nn.Linear(l1, l2)\n",
    "        self.fc3 = nn.Linear(l2, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5) # Same as flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cifar(config): # Function API trainable\n",
    "    net = Net(config[\"l1\"], config[\"l2\"])\n",
    "\n",
    "    # device = \"cpu\"\n",
    "    # if torch.cuda.is_available():\n",
    "    #     device = \"cuda:0\"\n",
    "    #     if torch.cuda.device_count() > 1:\n",
    "    #         net = nn.DataParallel(net)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    net.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # nn.module.parameters() returns an iterator over the module's parameters, it is typically passed to an optimizer.\n",
    "    # SGD optimizer\n",
    "    optimizer = optim.SGD(net.parameters(), lr=config[\"lr\"], momentum=0.9)\n",
    "\n",
    "    # Load existing checkpoint through `get_checkpoint()` API.\n",
    "    if train.get_checkpoint():\n",
    "        loaded_checkpoint = train.get_checkpoint()\n",
    "        with loaded_checkpoint.as_directory() as loaded_checkpoint_dir:\n",
    "            model_state, optimizer_state = torch.load(\n",
    "                os.path.join(loaded_checkpoint_dir, \"checkpoint.pt\")\n",
    "            )\n",
    "            net.load_state_dict(model_state)\n",
    "            optimizer.load_state_dict(optimizer_state)\n",
    "\n",
    "    if config[\"smoke_test\"]:\n",
    "        trainset, _ = load_test_data()\n",
    "    else:\n",
    "        trainset, _ = load_data()\n",
    "\n",
    "    test_abs = int(len(trainset) * 0.8)\n",
    "    train_subset, val_subset = random_split(\n",
    "        trainset, [test_abs, len(trainset) - test_abs])\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        train_subset,\n",
    "        batch_size=int(config[\"batch_size\"]),\n",
    "        shuffle=True,\n",
    "        num_workers=0 if config[\"smoke_test\"] else 8,\n",
    "    )\n",
    "    valloader = torch.utils.data.DataLoader(\n",
    "        val_subset,\n",
    "        batch_size=int(config[\"batch_size\"]),\n",
    "        shuffle=True,\n",
    "        num_workers=0 if config[\"smoke_test\"] else 8,\n",
    "    )\n",
    "\n",
    "    for epoch in range(10):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        epoch_steps = 0\n",
    "        for i, data in enumerate(trainloader):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad() # Because pytorch accumulates the gradients on subsequent backward passes, which is convenient for RNNs. But not for CNNs.\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward() # TODO: What does this do?\n",
    "            optimizer.step() # TODO: What does this do?\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            epoch_steps += 1\n",
    "            if i % 2000 == 1999:  # print every 2000 mini-batches\n",
    "                print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1,\n",
    "                                                running_loss / epoch_steps))\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # Validation loss\n",
    "        val_loss = 0.0\n",
    "        val_steps = 0\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for i, data in enumerate(valloader, 0):\n",
    "            with torch.no_grad():\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = net(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.cpu().numpy()\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "                val_steps += 1\n",
    "\n",
    "        precision = precision_score(val_labels, val_preds, average='weighted', zero_division=0)\n",
    "        recall = recall_score(val_labels, val_preds, average='weighted', zero_division=0)\n",
    "        f1 = f1_score(val_labels, val_preds, average='weighted', zero_division=0)\n",
    "        \n",
    "        # Here we save a checkpoint. It is automatically registered with\n",
    "        # Ray Tune and will potentially be accessed through in ``get_checkpoint()``\n",
    "        # in future iterations.\n",
    "        # Note to save a file like checkpoint, you still need to put it under a directory\n",
    "        # to construct a checkpoint.\n",
    "        with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n",
    "            path = os.path.join(temp_checkpoint_dir, \"checkpoint.pt\")\n",
    "            torch.save(\n",
    "                (net.state_dict(), optimizer.state_dict()), path\n",
    "            )\n",
    "            checkpoint = Checkpoint.from_directory(temp_checkpoint_dir)\n",
    "            #----- Report Score to Ray Tune -----\n",
    "            train.report(\n",
    "                {\"loss\": (val_loss / val_steps), \"accuracy\": correct / total, \"precision\": precision, \"recall\": recall, \"f1\": f1},\n",
    "                checkpoint=checkpoint,\n",
    "            )\n",
    "    print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_best_model(best_result, smoke_test=False):\n",
    "    best_trained_model = Net(best_result.config[\"l1\"], best_result.config[\"l2\"])\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    best_trained_model.to(device)\n",
    "\n",
    "    checkpoint_path = os.path.join(best_result.checkpoint.to_directory(), \"checkpoint.pt\")\n",
    "\n",
    "    model_state, optimizer_state = torch.load(checkpoint_path)\n",
    "    best_trained_model.load_state_dict(model_state)\n",
    "\n",
    "    if smoke_test:\n",
    "        _, testset = load_test_data()\n",
    "    else:\n",
    "        _, testset = load_data()\n",
    "\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=4, shuffle=False, num_workers=2\n",
    "    )\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_preds = []\n",
    "    test_labels = []\n",
    "    with torch.no_grad(): # We don't need to calculate the gradients when testing\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = best_trained_model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            test_preds.extend(preds.cpu().numpy())\n",
    "            test_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    precision = precision_score(test_labels, test_preds, average='weighted')\n",
    "    recall = recall_score(test_labels, test_preds, average='weighted')\n",
    "    f1 = f1_score(test_labels, test_preds, average='weighted')\n",
    "    print(\"Best trial test set accuracy: {}\".format(correct / total))\n",
    "    print(\"Best trial test set precision: {}\".format(precision))\n",
    "    print(\"Best trial test set recall: {}\".format(recall))\n",
    "    print(\"Best trial test set f1: {}\".format(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-09-10 21:55:24</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:08.83        </td></tr>\n",
       "<tr><td>Memory:      </td><td>5.6/31.2 GiB       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=9<br>Bracket: Iter 8.000: None | Iter 4.000: -2.2826595306396484 | Iter 2.000: -2.3006701469421387 | Iter 1.000: -2.3112635016441345<br>Logical resource usage: 6.0/28 CPUs, 0.25/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status    </th><th>loc                 </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   loss</th><th style=\"text-align: right;\">  accuracy</th><th style=\"text-align: right;\">  precision</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_cifar_08c38_00000</td><td>TERMINATED</td><td>172.30.197.30:195293</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">0.0285523  </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        0.551997</td><td style=\"text-align: right;\">2.31532</td><td style=\"text-align: right;\"> 0.115385 </td><td style=\"text-align: right;\"> 0.0133136 </td></tr>\n",
       "<tr><td>train_cifar_08c38_00001</td><td>TERMINATED</td><td>172.30.197.30:195294</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">0.000935676</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.451533</td><td style=\"text-align: right;\">2.31188</td><td style=\"text-align: right;\"> 0.0769231</td><td style=\"text-align: right;\"> 0.00591716</td></tr>\n",
       "<tr><td>train_cifar_08c38_00002</td><td>TERMINATED</td><td>172.30.197.30:195295</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">0.000122106</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">        0.61636 </td><td style=\"text-align: right;\">2.27455</td><td style=\"text-align: right;\"> 0.230769 </td><td style=\"text-align: right;\"> 0.0532544 </td></tr>\n",
       "<tr><td>train_cifar_08c38_00003</td><td>TERMINATED</td><td>172.30.197.30:195296</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">0.000793641</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.435413</td><td style=\"text-align: right;\">2.32098</td><td style=\"text-align: right;\"> 0.115385 </td><td style=\"text-align: right;\"> 0.0133136 </td></tr>\n",
       "<tr><td>train_cifar_08c38_00004</td><td>TERMINATED</td><td>172.30.197.30:195860</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">0.000156045</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.437824</td><td style=\"text-align: right;\">2.33   </td><td style=\"text-align: right;\"> 0.0384615</td><td style=\"text-align: right;\"> 0.00147929</td></tr>\n",
       "<tr><td>train_cifar_08c38_00005</td><td>TERMINATED</td><td>172.30.197.30:195861</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">0.0292311  </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        0.488307</td><td style=\"text-align: right;\">2.2928 </td><td style=\"text-align: right;\"> 0.0769231</td><td style=\"text-align: right;\"> 0.00591716</td></tr>\n",
       "<tr><td>train_cifar_08c38_00006</td><td>TERMINATED</td><td>172.30.197.30:195862</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">0.00128328 </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        0.495528</td><td style=\"text-align: right;\">2.29078</td><td style=\"text-align: right;\"> 0.0769231</td><td style=\"text-align: right;\"> 0.00591716</td></tr>\n",
       "<tr><td>train_cifar_08c38_00007</td><td>TERMINATED</td><td>172.30.197.30:195863</td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">0.000428544</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.454755</td><td style=\"text-align: right;\">2.3117 </td><td style=\"text-align: right;\"> 0.153846 </td><td style=\"text-align: right;\"> 0.0246154 </td></tr>\n",
       "<tr><td>train_cifar_08c38_00008</td><td>TERMINATED</td><td>172.30.197.30:196241</td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">0.000158407</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        0.376354</td><td style=\"text-align: right;\">2.31101</td><td style=\"text-align: right;\"> 0.0384615</td><td style=\"text-align: right;\"> 0.00147929</td></tr>\n",
       "<tr><td>train_cifar_08c38_00009</td><td>TERMINATED</td><td>172.30.197.30:196242</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">0.000127323</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        0.349763</td><td style=\"text-align: right;\">2.30854</td><td style=\"text-align: right;\"> 0.115385 </td><td style=\"text-align: right;\"> 0.0133136 </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_cifar pid=195295)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/yuhang/ray_results/train_cifar_2024-09-10_21-55-13/train_cifar_08c38_00002_2_batch_size=64,lr=0.0001_2024-09-10_21-55-15/checkpoint_000000)\n",
      "\u001b[36m(train_cifar pid=195295)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/yuhang/ray_results/train_cifar_2024-09-10_21-55-13/train_cifar_08c38_00002_2_batch_size=64,lr=0.0001_2024-09-10_21-55-15/checkpoint_000001)\n",
      "\u001b[36m(train_cifar pid=195295)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/yuhang/ray_results/train_cifar_2024-09-10_21-55-13/train_cifar_08c38_00002_2_batch_size=64,lr=0.0001_2024-09-10_21-55-15/checkpoint_000002)\n",
      "\u001b[36m(train_cifar pid=196241)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/yuhang/ray_results/train_cifar_2024-09-10_21-55-13/train_cifar_08c38_00008_8_batch_size=8,lr=0.0002_2024-09-10_21-55-15/checkpoint_000001)\u001b[32m [repeated 16x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "2024-09-10 21:55:24,168\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/yuhang/ray_results/train_cifar_2024-09-10_21-55-13' in 0.0097s.\n",
      "2024-09-10 21:55:24,172\tINFO tune.py:1041 -- Total run time: 9.05 seconds (8.82 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial config: {'l1': 64, 'l2': 16, 'lr': 0.00012210592916273645, 'batch_size': 64, 'smoke_test': True}\n",
      "Best trial final validation loss: 2.274550199508667\n",
      "Best trial final validation accuracy: 0.23076923076923078\n",
      "Best trial final validation precision: 0.05325443786982249\n",
      "Best trial final validation recall: 0.23076923076923078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_193188/576389788.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_state, optimizer_state = torch.load(checkpoint_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial test set accuracy: 0.0625\n",
      "Best trial test set precision: 0.00390625\n",
      "Best trial test set recall: 0.0625\n",
      "Best trial test set f1: 0.007352941176470588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuhang/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def main(num_samples=10, max_num_epochs=10, gpus_per_trial=1, smoke_test=False):\n",
    "    config = {\n",
    "        \"l1\": tune.sample_from(lambda _: 2 ** np.random.randint(4, 9)),\n",
    "        \"l2\": tune.sample_from(lambda _: 2 ** np.random.randint(4, 9)),\n",
    "        \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "        \"batch_size\": tune.choice([8, 16, 32, 64]),\n",
    "        \"smoke_test\": smoke_test,\n",
    "    }\n",
    "    scheduler = ASHAScheduler( #Schedule is used to stop searches early\n",
    "        max_t=max_num_epochs,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2)\n",
    "    tuner = tune.Tuner(\n",
    "        tune.with_resources(\n",
    "            tune.with_parameters(train_cifar), # train_cifar is the trainable function\n",
    "            resources={\"cpu\": 6, \"gpu\": gpus_per_trial} # Per trail resource\n",
    "        ),\n",
    "        tune_config=tune.TuneConfig(\n",
    "            metric=\"loss\",\n",
    "            mode=\"min\",\n",
    "            scheduler=scheduler,\n",
    "            num_samples=num_samples,\n",
    "        ),\n",
    "        run_config=train.RunConfig(stop={\"training_iteration\": 5}),\n",
    "        param_space=config,\n",
    "    )\n",
    "    results = tuner.fit()\n",
    "    \n",
    "    best_result = results.get_best_result(\"loss\", \"min\")\n",
    "\n",
    "    print(\"Best trial config: {}\".format(best_result.config))\n",
    "    print(\"Best trial final validation loss: {}\".format(best_result.metrics[\"loss\"]))\n",
    "    print(\"Best trial final validation accuracy: {}\".format(best_result.metrics[\"accuracy\"]))\n",
    "    print(\"Best trial final validation precision: {}\".format(best_result.metrics[\"precision\"]))\n",
    "    print(\"Best trial final validation recall: {}\".format(best_result.metrics[\"recall\"]))\n",
    "\n",
    "    # Testdata\n",
    "    test_best_model(best_result, smoke_test=smoke_test) \n",
    "    return results\n",
    "\n",
    "# The gpus_per_trail can be fractional, e.g. 0.5, just make sure GPU has enpugh memory.\n",
    "# num_samples is the number of sample from hyperparameter space. = -1 means infinite samples until a stopping condition is met.\n",
    "result =  main(num_samples=10, max_num_epochs=10, gpus_per_trial=0.25, smoke_test=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResultGrid<[\n",
       "  Result(\n",
       "    metrics={'loss': 2.3153200149536133, 'accuracy': 0.11538461538461539, 'precision': 0.013313609467455622, 'recall': 0.11538461538461539, 'f1': 0.02387267904509284},\n",
       "    path='/home/yuhang/ray_results/train_cifar_2024-09-10_21-55-13/train_cifar_08c38_00000_0_batch_size=32,lr=0.0286_2024-09-10_21-55-15',\n",
       "    filesystem='local',\n",
       "    checkpoint=Checkpoint(filesystem=local, path=/home/yuhang/ray_results/train_cifar_2024-09-10_21-55-13/train_cifar_08c38_00000_0_batch_size=32,lr=0.0286_2024-09-10_21-55-15/checkpoint_000001)\n",
       "  ),\n",
       "  Result(\n",
       "    metrics={'loss': 2.3118847608566284, 'accuracy': 0.07692307692307693, 'precision': 0.00591715976331361, 'recall': 0.07692307692307693, 'f1': 0.01098901098901099},\n",
       "    path='/home/yuhang/ray_results/train_cifar_2024-09-10_21-55-13/train_cifar_08c38_00001_1_batch_size=16,lr=0.0009_2024-09-10_21-55-15',\n",
       "    filesystem='local',\n",
       "    checkpoint=Checkpoint(filesystem=local, path=/home/yuhang/ray_results/train_cifar_2024-09-10_21-55-13/train_cifar_08c38_00001_1_batch_size=16,lr=0.0009_2024-09-10_21-55-15/checkpoint_000000)\n",
       "  ),\n",
       "  Result(\n",
       "    metrics={'loss': 2.274550199508667, 'accuracy': 0.23076923076923078, 'precision': 0.05325443786982249, 'recall': 0.23076923076923078, 'f1': 0.08653846153846154},\n",
       "    path='/home/yuhang/ray_results/train_cifar_2024-09-10_21-55-13/train_cifar_08c38_00002_2_batch_size=64,lr=0.0001_2024-09-10_21-55-15',\n",
       "    filesystem='local',\n",
       "    checkpoint=Checkpoint(filesystem=local, path=/home/yuhang/ray_results/train_cifar_2024-09-10_21-55-13/train_cifar_08c38_00002_2_batch_size=64,lr=0.0001_2024-09-10_21-55-15/checkpoint_000004)\n",
       "  ),\n",
       "  Result(\n",
       "    metrics={'loss': 2.320977210998535, 'accuracy': 0.11538461538461539, 'precision': 0.013313609467455622, 'recall': 0.11538461538461539, 'f1': 0.02387267904509284},\n",
       "    path='/home/yuhang/ray_results/train_cifar_2024-09-10_21-55-13/train_cifar_08c38_00003_3_batch_size=64,lr=0.0008_2024-09-10_21-55-15',\n",
       "    filesystem='local',\n",
       "    checkpoint=Checkpoint(filesystem=local, path=/home/yuhang/ray_results/train_cifar_2024-09-10_21-55-13/train_cifar_08c38_00003_3_batch_size=64,lr=0.0008_2024-09-10_21-55-15/checkpoint_000000)\n",
       "  ),\n",
       "  Result(\n",
       "    metrics={'loss': 2.3300037384033203, 'accuracy': 0.038461538461538464, 'precision': 0.0014792899408284025, 'recall': 0.038461538461538464, 'f1': 0.0028490028490028487},\n",
       "    path='/home/yuhang/ray_results/train_cifar_2024-09-10_21-55-13/train_cifar_08c38_00004_4_batch_size=32,lr=0.0002_2024-09-10_21-55-15',\n",
       "    filesystem='local',\n",
       "    checkpoint=Checkpoint(filesystem=local, path=/home/yuhang/ray_results/train_cifar_2024-09-10_21-55-13/train_cifar_08c38_00004_4_batch_size=32,lr=0.0002_2024-09-10_21-55-15/checkpoint_000000)\n",
       "  ),\n",
       "  Result(\n",
       "    metrics={'loss': 2.292799949645996, 'accuracy': 0.07692307692307693, 'precision': 0.00591715976331361, 'recall': 0.07692307692307693, 'f1': 0.01098901098901099},\n",
       "    path='/home/yuhang/ray_results/train_cifar_2024-09-10_21-55-13/train_cifar_08c38_00005_5_batch_size=32,lr=0.0292_2024-09-10_21-55-15',\n",
       "    filesystem='local',\n",
       "    checkpoint=Checkpoint(filesystem=local, path=/home/yuhang/ray_results/train_cifar_2024-09-10_21-55-13/train_cifar_08c38_00005_5_batch_size=32,lr=0.0292_2024-09-10_21-55-15/checkpoint_000001)\n",
       "  ),\n",
       "  Result(\n",
       "    metrics={'loss': 2.2907803058624268, 'accuracy': 0.07692307692307693, 'precision': 0.00591715976331361, 'recall': 0.07692307692307693, 'f1': 0.01098901098901099},\n",
       "    path='/home/yuhang/ray_results/train_cifar_2024-09-10_21-55-13/train_cifar_08c38_00006_6_batch_size=64,lr=0.0013_2024-09-10_21-55-15',\n",
       "    filesystem='local',\n",
       "    checkpoint=Checkpoint(filesystem=local, path=/home/yuhang/ray_results/train_cifar_2024-09-10_21-55-13/train_cifar_08c38_00006_6_batch_size=64,lr=0.0013_2024-09-10_21-55-15/checkpoint_000003)\n",
       "  ),\n",
       "  Result(\n",
       "    metrics={'loss': 2.311699151992798, 'accuracy': 0.15384615384615385, 'precision': 0.024615384615384615, 'recall': 0.15384615384615385, 'f1': 0.04244031830238727},\n",
       "    path='/home/yuhang/ray_results/train_cifar_2024-09-10_21-55-13/train_cifar_08c38_00007_7_batch_size=8,lr=0.0004_2024-09-10_21-55-15',\n",
       "    filesystem='local',\n",
       "    checkpoint=Checkpoint(filesystem=local, path=/home/yuhang/ray_results/train_cifar_2024-09-10_21-55-13/train_cifar_08c38_00007_7_batch_size=8,lr=0.0004_2024-09-10_21-55-15/checkpoint_000000)\n",
       "  ),\n",
       "  Result(\n",
       "    metrics={'loss': 2.31101393699646, 'accuracy': 0.038461538461538464, 'precision': 0.0014792899408284025, 'recall': 0.038461538461538464, 'f1': 0.0028490028490028487},\n",
       "    path='/home/yuhang/ray_results/train_cifar_2024-09-10_21-55-13/train_cifar_08c38_00008_8_batch_size=8,lr=0.0002_2024-09-10_21-55-15',\n",
       "    filesystem='local',\n",
       "    checkpoint=Checkpoint(filesystem=local, path=/home/yuhang/ray_results/train_cifar_2024-09-10_21-55-13/train_cifar_08c38_00008_8_batch_size=8,lr=0.0002_2024-09-10_21-55-15/checkpoint_000001)\n",
       "  ),\n",
       "  Result(\n",
       "    metrics={'loss': 2.3085403442382812, 'accuracy': 0.11538461538461539, 'precision': 0.013313609467455622, 'recall': 0.11538461538461539, 'f1': 0.02387267904509284},\n",
       "    path='/home/yuhang/ray_results/train_cifar_2024-09-10_21-55-13/train_cifar_08c38_00009_9_batch_size=32,lr=0.0001_2024-09-10_21-55-15',\n",
       "    filesystem='local',\n",
       "    checkpoint=Checkpoint(filesystem=local, path=/home/yuhang/ray_results/train_cifar_2024-09-10_21-55-13/train_cifar_08c38_00009_9_batch_size=32,lr=0.0001_2024-09-10_21-55-15/checkpoint_000001)\n",
       "  )\n",
       "]>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/yuhang/anaconda3/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/yuhang/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/yuhang/anaconda3/lib/python3.9/site-packages/wandb/__main__.py\", line 1, in <module>\n",
      "    from wandb.cli import cli\n",
      "  File \"/home/yuhang/anaconda3/lib/python3.9/site-packages/wandb/cli/cli.py\", line 65, in <module>\n",
      "    logging.basicConfig(\n",
      "  File \"/home/yuhang/anaconda3/lib/python3.9/logging/__init__.py\", line 2003, in basicConfig\n",
      "    h = FileHandler(filename, mode,\n",
      "  File \"/home/yuhang/anaconda3/lib/python3.9/logging/__init__.py\", line 1146, in __init__\n",
      "    StreamHandler.__init__(self, self._open())\n",
      "  File \"/home/yuhang/anaconda3/lib/python3.9/logging/__init__.py\", line 1175, in _open\n",
      "    return open(self.baseFilename, self.mode, encoding=self.encoding,\n",
      "PermissionError: [Errno 13] Permission denied: '/home/yuhang/repos/cnn_tutorial/wandb/debug-cli.yuhang.log'\n"
     ]
    },
    {
     "ename": "ServiceStartProcessError",
     "evalue": "The wandb service process exited with 1. Ensure that `sys.executable` is a valid python interpreter. You can override it with the `_executable` setting or with the `WANDB__EXECUTABLE` environment variable.\n{'command': ['/home/yuhang/anaconda3/bin/python', '-m', 'wandb', 'service', '--debug', '--port-filename', '/tmp/tmpcqoe1oec/port-193188.txt', '--pid', '193188', '--serve-sock'], 'sys_executable': '/home/yuhang/anaconda3/bin/python', 'which_python': '/home/yuhang/anaconda3/bin/python3', 'proc_out': '', 'proc_err': ''}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mServiceStartProcessError\u001b[0m                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWANDB_NOTEBOOK_NAME\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mray_torch_example_SGD.ipynb\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      5\u001b[0m notes \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRay_Tune_Test\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m run \u001b[38;5;241m=\u001b[39m \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcifar10\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnotes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnotes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRay_tune\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcifar10\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAuto hyperparameter settings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/wandb/sdk/wandb_init.py:1239\u001b[0m, in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, fork_from, resume_from, settings)\u001b[0m\n\u001b[1;32m   1235\u001b[0m     logger\u001b[38;5;241m.\u001b[39mexception(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror in wandb.init()\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39me)\n\u001b[1;32m   1237\u001b[0m \u001b[38;5;66;03m# Need to build delay into this sentry capture because our exit hooks\u001b[39;00m\n\u001b[1;32m   1238\u001b[0m \u001b[38;5;66;03m# mess with sentry's ability to send out errors before the program ends.\u001b[39;00m\n\u001b[0;32m-> 1239\u001b[0m \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sentry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1240\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/wandb/analytics/sentry.py:155\u001b[0m, in \u001b[0;36mSentry.reraise\u001b[0;34m(self, exc)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexception(exc)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# this will messily add this \"reraise\" function to the stack trace,\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;66;03m# but hopefully it's not too bad\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mwith_traceback(sys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/wandb/sdk/wandb_init.py:1224\u001b[0m, in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, fork_from, resume_from, settings)\u001b[0m\n\u001b[1;32m   1222\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1223\u001b[0m     wi \u001b[38;5;241m=\u001b[39m _WandbInit()\n\u001b[0;32m-> 1224\u001b[0m     \u001b[43mwi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m wi\u001b[38;5;241m.\u001b[39minit()\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/wandb/sdk/wandb_init.py:184\u001b[0m, in \u001b[0;36m_WandbInit.setup\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m _disable_service \u001b[38;5;241m=\u001b[39m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisabled\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m settings_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisabled\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    182\u001b[0m setup_settings \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_disable_service\u001b[39m\u001b[38;5;124m\"\u001b[39m: _disable_service}\n\u001b[0;32m--> 184\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wl \u001b[38;5;241m=\u001b[39m \u001b[43mwandb_setup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43msettings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msetup_settings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# Make sure we have a logger setup (might be an early logger)\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/wandb/sdk/wandb_setup.py:378\u001b[0m, in \u001b[0;36msetup\u001b[0;34m(settings)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msetup\u001b[39m(settings: Optional[Settings] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_WandbSetup\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;124;03m\"\"\"Prepares W&B for use in the current process and its children.\u001b[39;00m\n\u001b[1;32m    324\u001b[0m \n\u001b[1;32m    325\u001b[0m \u001b[38;5;124;03m    You can usually ignore this as it is implicitly called by `wandb.init()`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;124;03m        ```\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 378\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43m_setup\u001b[49m\u001b[43m(\u001b[49m\u001b[43msettings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msettings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/wandb/sdk/wandb_setup.py:318\u001b[0m, in \u001b[0;36m_setup\u001b[0;34m(settings, _reset)\u001b[0m\n\u001b[1;32m    315\u001b[0m     teardown()\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 318\u001b[0m wl \u001b[38;5;241m=\u001b[39m \u001b[43m_WandbSetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43msettings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msettings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wl\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/wandb/sdk/wandb_setup.py:303\u001b[0m, in \u001b[0;36m_WandbSetup.__init__\u001b[0;34m(self, settings)\u001b[0m\n\u001b[1;32m    301\u001b[0m     _WandbSetup\u001b[38;5;241m.\u001b[39m_instance\u001b[38;5;241m.\u001b[39m_update(settings\u001b[38;5;241m=\u001b[39msettings)\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 303\u001b[0m _WandbSetup\u001b[38;5;241m.\u001b[39m_instance \u001b[38;5;241m=\u001b[39m \u001b[43m_WandbSetup__WandbSetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43msettings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msettings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpid\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/wandb/sdk/wandb_setup.py:114\u001b[0m, in \u001b[0;36m_WandbSetup__WandbSetup.__init__\u001b[0;34m(self, pid, settings, environ)\u001b[0m\n\u001b[1;32m    111\u001b[0m wandb\u001b[38;5;241m.\u001b[39mtermsetup(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_settings, logger)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check()\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m tracelog_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_settings\u001b[38;5;241m.\u001b[39m_tracelog\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tracelog_mode:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/wandb/sdk/wandb_setup.py:250\u001b[0m, in \u001b[0;36m_WandbSetup__WandbSetup._setup\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_setup\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 250\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_manager\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m     sweep_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_settings\u001b[38;5;241m.\u001b[39msweep_param_path\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sweep_path:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/wandb/sdk/wandb_setup.py:283\u001b[0m, in \u001b[0;36m_WandbSetup__WandbSetup._setup_manager\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_settings\u001b[38;5;241m.\u001b[39m_disable_service:\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 283\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_manager \u001b[38;5;241m=\u001b[39m \u001b[43mwandb_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Manager\u001b[49m\u001b[43m(\u001b[49m\u001b[43msettings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_settings\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/wandb/sdk/wandb_manager.py:145\u001b[0m, in \u001b[0;36m_Manager.__init__\u001b[0;34m(self, settings)\u001b[0m\n\u001b[1;32m    143\u001b[0m token \u001b[38;5;241m=\u001b[39m _ManagerToken\u001b[38;5;241m.\u001b[39mfrom_environment()\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m token:\n\u001b[0;32m--> 145\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_service\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m     host \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocalhost\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    147\u001b[0m     transport \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtcp\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/wandb/sdk/service/service.py:253\u001b[0m, in \u001b[0;36m_Service.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstart\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 253\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/wandb/sdk/service/service.py:247\u001b[0m, in \u001b[0;36m_Service._launch_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_ports(fname, proc\u001b[38;5;241m=\u001b[39minternal_proc)\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 247\u001b[0m     \u001b[43m_sentry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_startup_debug_print(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwait_ports_done\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_proc \u001b[38;5;241m=\u001b[39m internal_proc\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/wandb/analytics/sentry.py:155\u001b[0m, in \u001b[0;36mSentry.reraise\u001b[0;34m(self, exc)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexception(exc)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# this will messily add this \"reraise\" function to the stack trace,\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;66;03m# but hopefully it's not too bad\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mwith_traceback(sys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/wandb/sdk/service/service.py:245\u001b[0m, in \u001b[0;36m_Service._launch_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_startup_debug_print(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwait_ports\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 245\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_ports\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minternal_proc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    247\u001b[0m     _sentry\u001b[38;5;241m.\u001b[39mreraise(e)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/wandb/sdk/service/service.py:109\u001b[0m, in \u001b[0;36m_Service._wait_for_ports\u001b[0;34m(self, fname, proc)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m proc \u001b[38;5;129;01mand\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mpoll():\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;66;03m# process finished\u001b[39;00m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;66;03m# define these variables for sentry context grab:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m# proc_out = proc.stdout.read()\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;66;03m# proc_err = proc.stderr.read()\u001b[39;00m\n\u001b[1;32m    102\u001b[0m     context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m    103\u001b[0m         command\u001b[38;5;241m=\u001b[39mproc\u001b[38;5;241m.\u001b[39margs,\n\u001b[1;32m    104\u001b[0m         sys_executable\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mexecutable,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    107\u001b[0m         proc_err\u001b[38;5;241m=\u001b[39mproc\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mread() \u001b[38;5;28;01mif\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    108\u001b[0m     )\n\u001b[0;32m--> 109\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ServiceStartProcessError(\n\u001b[1;32m    110\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe wandb service process exited with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproc\u001b[38;5;241m.\u001b[39mreturncode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    111\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnsure that `sys.executable` is a valid python interpreter. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    112\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can override it with the `_executable` setting \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor with the `WANDB__EXECUTABLE` environment variable.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    115\u001b[0m         context\u001b[38;5;241m=\u001b[39mcontext,\n\u001b[1;32m    116\u001b[0m     )\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(fname):\n\u001b[1;32m    118\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.2\u001b[39m)\n",
      "\u001b[0;31mServiceStartProcessError\u001b[0m: The wandb service process exited with 1. Ensure that `sys.executable` is a valid python interpreter. You can override it with the `_executable` setting or with the `WANDB__EXECUTABLE` environment variable.\n{'command': ['/home/yuhang/anaconda3/bin/python', '-m', 'wandb', 'service', '--debug', '--port-filename', '/tmp/tmpcqoe1oec/port-193188.txt', '--pid', '193188', '--serve-sock'], 'sys_executable': '/home/yuhang/anaconda3/bin/python', 'which_python': '/home/yuhang/anaconda3/bin/python3', 'proc_out': '', 'proc_err': ''}"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import os\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'ray_torch_example_SGD.ipynb'\n",
    "\n",
    "notes = \"Ray_Tune_Test\"\n",
    "run = wandb.init(project='cifar10', notes=notes, tags=['Ray_tune', 'cifar10', \"Auto hyperparameter settings\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
